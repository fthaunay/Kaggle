{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "resources = pd.read_csv(\"resources.csv\")\n",
    "train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p233245</td>\n",
       "      <td>LC652 - Lakeshore Double-Space Mobile Drying Rack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p069063</td>\n",
       "      <td>Bouncy Bands for Desks (Blue support pipes)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p069063</td>\n",
       "      <td>Cory Stories: A Kid's Book About Living With Adhd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p069063</td>\n",
       "      <td>Dixon Ticonderoga Wood-Cased #2 HB Pencils, Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p069063</td>\n",
       "      <td>EDUCATIONAL INSIGHTS FLUORESCENT LIGHT FILTERS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                        description\n",
       "0  p233245  LC652 - Lakeshore Double-Space Mobile Drying Rack\n",
       "1  p069063        Bouncy Bands for Desks (Blue support pipes)\n",
       "2  p069063  Cory Stories: A Kid's Book About Living With Adhd\n",
       "3  p069063  Dixon Ticonderoga Wood-Cased #2 HB Pencils, Bo...\n",
       "4  p069063  EDUCATIONAL INSIGHTS FLUORESCENT LIGHT FILTERS..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resources = resources[[\"id\",\"description\"]]\n",
    "resources.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()\n",
    "train = train[['project_title', 'project_essay_1', 'project_essay_2',\n",
    "       'project_essay_3', 'project_essay_4', 'project_resource_summary', 'project_is_approved']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_essay_1</th>\n",
       "      <th>project_essay_2</th>\n",
       "      <th>project_essay_3</th>\n",
       "      <th>project_essay_4</th>\n",
       "      <th>project_resource_summary</th>\n",
       "      <th>project_is_approved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Super Sight Word Centers</td>\n",
       "      <td>Most of my kindergarten students come from low...</td>\n",
       "      <td>I currently have a differentiated sight word c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My students need 6 Ipod Nano's to create and d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Keep Calm and Dance On</td>\n",
       "      <td>Our elementary school is a culturally rich sch...</td>\n",
       "      <td>We strive to provide our diverse population of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My students need matching shirts to wear for d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lets 3Doodle to Learn</td>\n",
       "      <td>Hello;\\r\\nMy name is Mrs. Brotherton. I teach ...</td>\n",
       "      <td>We are looking to add some 3Doodler to our cla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My students need the 3doodler. We are an SEM s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\\"Kid Inspired\\\" Equipment to Increase Activit...</td>\n",
       "      <td>My students are the greatest students but are ...</td>\n",
       "      <td>The student's project which is totally \\\"kid-i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My students need balls and other activity equi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We need clean water for our culinary arts class!</td>\n",
       "      <td>My students are athletes and students who are ...</td>\n",
       "      <td>For some reason in our kitchen the water comes...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My students need a water filtration system for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       project_title  \\\n",
       "0                           Super Sight Word Centers   \n",
       "1                             Keep Calm and Dance On   \n",
       "2                              Lets 3Doodle to Learn   \n",
       "3  \\\"Kid Inspired\\\" Equipment to Increase Activit...   \n",
       "4   We need clean water for our culinary arts class!   \n",
       "\n",
       "                                     project_essay_1  \\\n",
       "0  Most of my kindergarten students come from low...   \n",
       "1  Our elementary school is a culturally rich sch...   \n",
       "2  Hello;\\r\\nMy name is Mrs. Brotherton. I teach ...   \n",
       "3  My students are the greatest students but are ...   \n",
       "4  My students are athletes and students who are ...   \n",
       "\n",
       "                                     project_essay_2 project_essay_3  \\\n",
       "0  I currently have a differentiated sight word c...             NaN   \n",
       "1  We strive to provide our diverse population of...             NaN   \n",
       "2  We are looking to add some 3Doodler to our cla...             NaN   \n",
       "3  The student's project which is totally \\\"kid-i...             NaN   \n",
       "4  For some reason in our kitchen the water comes...             NaN   \n",
       "\n",
       "  project_essay_4                           project_resource_summary  \\\n",
       "0             NaN  My students need 6 Ipod Nano's to create and d...   \n",
       "1             NaN  My students need matching shirts to wear for d...   \n",
       "2             NaN  My students need the 3doodler. We are an SEM s...   \n",
       "3             NaN  My students need balls and other activity equi...   \n",
       "4             NaN  My students need a water filtration system for...   \n",
       "\n",
       "   project_is_approved  \n",
       "0                    1  \n",
       "1                    0  \n",
       "2                    1  \n",
       "3                    0  \n",
       "4                    1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"My students need 6 Ipod Nano's to create and differentiated and engaging way to practice sight words during a literacy station.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"project_resource_summary\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Super Sight Word Centers'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"project_title\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Most of my kindergarten students come from low-income households and are considered \\\\\"at-risk\\\\\". These kids walk to school alongside their parents and most have never been further than walking distance from their house. For 80% of my students, English is not their first language or the language spoken at home. \\\\r\\\\n\\\\r\\\\nWhile my kindergarten kids have many obstacles in front of them, they come to school each day excited and ready to learn. Most students started the year out never being in a school setting. At the start of the year many had never been exposed to letters. Each day they soak up more knowledge and try their hardest to succeed. They are highly motivated to learn new things every day. We are halfway through the year and they are starting to take off. They know know all letters, some sight words, numbers to 20, and a majority of their letter sounds because of their hard work and determination. I am excited to see the places we will go from here!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"project_essay_1\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LC652 - Lakeshore Double-Space Mobile Drying Rack'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resources[\"description\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initate stemmers\n",
    "french_stem = FrenchStemmer(ignore_stopwords=True)\n",
    "english_stem = EnglishStemmer(ignore_stopwords=True)\n",
    "german_stem = GermanStemmer(ignore_stopwords=True)\n",
    "\n",
    "\n",
    "def stem_word(word):\n",
    "    \"\"\"\n",
    "    Remove the suffix of word using a stemmer from the most probable\n",
    "    to the least probable language.\n",
    "    :param word: string\n",
    "    :return: string\n",
    "    \"\"\"\n",
    "    y = french_stem.stem(word)\n",
    "\n",
    "    #if the word is not french, it may be english\n",
    "    if word != y:\n",
    "        return y\n",
    "    else:\n",
    "        w = english_stem.stem(word)\n",
    "        if word != w:\n",
    "            return w\n",
    "        else:\n",
    "            #if not english it may be german\n",
    "            z = german_stem.stem(word)\n",
    "            if word != z:\n",
    "                return z\n",
    "            else:\n",
    "                return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_clean(note):\n",
    "    \"\"\"\n",
    "    The string \"note\" is split. First names are removed, words are stemmed,\n",
    "    numbers are removed and the numeric part of alphanumeric words are repalced\n",
    "    by a generic number.\n",
    "    :param note: A string\n",
    "    :return: A list of words\n",
    "    \"\"\"\n",
    "    # Regex pattern to match punctuation signs\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    elements = regex.sub(' ', note).split()\n",
    "\n",
    "    #Remove phone numbers and single letters\n",
    "    alphanum_words = [x.strip() for x in elements if (not x.isdigit() and len(x)>1)]\n",
    "\n",
    "    #Replace numbers in alphanum words\n",
    "    cleaned_alphanum_words = [re.sub(r'[0-9]','0',x) for x in alphanum_words]\n",
    "\n",
    "    #Stem words\n",
    "    return [stem_word(x) for x in cleaned_alphanum_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaning_notes(note_list):\n",
    "    \"\"\"\n",
    "    Processing notes text before counting\n",
    "    :param note_list: The list of notes from self.data\n",
    "    :return: A list of list of words\n",
    "    \"\"\"\n",
    "    #Set strings as unicode\n",
    "    u_note_list = [str(x) for x in note_list]\n",
    "\n",
    "    return map(split_and_clean, u_note_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['most',\n",
       "  'of',\n",
       "  'my',\n",
       "  'kindergart',\n",
       "  'student',\n",
       "  'com',\n",
       "  'from',\n",
       "  'low',\n",
       "  'incom',\n",
       "  'household',\n",
       "  'and',\n",
       "  'are',\n",
       "  'consid',\n",
       "  'at',\n",
       "  'risk',\n",
       "  'thes',\n",
       "  'kid',\n",
       "  'walk',\n",
       "  'to',\n",
       "  'school',\n",
       "  'alongsid',\n",
       "  'their',\n",
       "  'parent',\n",
       "  'and',\n",
       "  'most',\n",
       "  'hav',\n",
       "  'nev',\n",
       "  'been',\n",
       "  'furth',\n",
       "  'than',\n",
       "  'walk',\n",
       "  'distanc',\n",
       "  'from',\n",
       "  'their',\n",
       "  'hous',\n",
       "  'for',\n",
       "  'of',\n",
       "  'my',\n",
       "  'student',\n",
       "  'english',\n",
       "  'is',\n",
       "  'not',\n",
       "  'their',\n",
       "  'first',\n",
       "  'languag',\n",
       "  'or',\n",
       "  'the',\n",
       "  'languag',\n",
       "  'spok',\n",
       "  'at',\n",
       "  'hom',\n",
       "  'nwhil',\n",
       "  'my',\n",
       "  'kindergart',\n",
       "  'kid',\n",
       "  'hav',\n",
       "  'mani',\n",
       "  'obstacl',\n",
       "  'in',\n",
       "  'front',\n",
       "  'of',\n",
       "  'them',\n",
       "  'they',\n",
       "  'com',\n",
       "  'to',\n",
       "  'school',\n",
       "  'each',\n",
       "  'day',\n",
       "  'excit',\n",
       "  'and',\n",
       "  'readi',\n",
       "  'to',\n",
       "  'learn',\n",
       "  'most',\n",
       "  'student',\n",
       "  'start',\n",
       "  'the',\n",
       "  'year',\n",
       "  'out',\n",
       "  'nev',\n",
       "  'being',\n",
       "  'in',\n",
       "  'school',\n",
       "  'set',\n",
       "  'at',\n",
       "  'the',\n",
       "  'start',\n",
       "  'of',\n",
       "  'the',\n",
       "  'year',\n",
       "  'mani',\n",
       "  'had',\n",
       "  'nev',\n",
       "  'been',\n",
       "  'expos',\n",
       "  'to',\n",
       "  'letter',\n",
       "  'each',\n",
       "  'day',\n",
       "  'they',\n",
       "  'soak',\n",
       "  'up',\n",
       "  'mor',\n",
       "  'knowledg',\n",
       "  'and',\n",
       "  'tri',\n",
       "  'their',\n",
       "  'hard',\n",
       "  'to',\n",
       "  'succeed',\n",
       "  'they',\n",
       "  'are',\n",
       "  'high',\n",
       "  'motiv',\n",
       "  'to',\n",
       "  'learn',\n",
       "  'new',\n",
       "  'thing',\n",
       "  'everi',\n",
       "  'day',\n",
       "  'we',\n",
       "  'are',\n",
       "  'halfway',\n",
       "  'through',\n",
       "  'the',\n",
       "  'year',\n",
       "  'and',\n",
       "  'they',\n",
       "  'are',\n",
       "  'start',\n",
       "  'to',\n",
       "  'tak',\n",
       "  'off',\n",
       "  'they',\n",
       "  'know',\n",
       "  'know',\n",
       "  'all',\n",
       "  'letter',\n",
       "  'som',\n",
       "  'sight',\n",
       "  'word',\n",
       "  'number',\n",
       "  'to',\n",
       "  'and',\n",
       "  'major',\n",
       "  'of',\n",
       "  'their',\n",
       "  'let',\n",
       "  'sound',\n",
       "  'becaus',\n",
       "  'of',\n",
       "  'their',\n",
       "  'hard',\n",
       "  'work',\n",
       "  'and',\n",
       "  'determin',\n",
       "  'am',\n",
       "  'excit',\n",
       "  'to',\n",
       "  'se',\n",
       "  'the',\n",
       "  'plac',\n",
       "  'we',\n",
       "  'will',\n",
       "  'go',\n",
       "  'from',\n",
       "  'her'],\n",
       " ['our',\n",
       "  'elementari',\n",
       "  'school',\n",
       "  'is',\n",
       "  'cultur',\n",
       "  'rich',\n",
       "  'school',\n",
       "  'with',\n",
       "  'divers',\n",
       "  'popul',\n",
       "  'of',\n",
       "  'student',\n",
       "  'in',\n",
       "  'pre',\n",
       "  'through',\n",
       "  'sixth',\n",
       "  'grad',\n",
       "  'nour',\n",
       "  'titl',\n",
       "  'school',\n",
       "  'popul',\n",
       "  'has',\n",
       "  'of',\n",
       "  'student',\n",
       "  'qualifi',\n",
       "  'for',\n",
       "  'fre',\n",
       "  'or',\n",
       "  'reduc',\n",
       "  'price',\n",
       "  'lunch',\n",
       "  'and',\n",
       "  'high',\n",
       "  'concentr',\n",
       "  'of',\n",
       "  'english',\n",
       "  'learner',\n",
       "  'we',\n",
       "  'also',\n",
       "  'serv',\n",
       "  'two',\n",
       "  'fost',\n",
       "  'group',\n",
       "  'hom',\n",
       "  'for',\n",
       "  'temporari',\n",
       "  'and',\n",
       "  'long',\n",
       "  'term',\n",
       "  'plac',\n",
       "  'of',\n",
       "  'homeless',\n",
       "  'childr',\n",
       "  'nwe',\n",
       "  'do',\n",
       "  'not',\n",
       "  'se',\n",
       "  'thes',\n",
       "  'statistic',\n",
       "  'as',\n",
       "  'road',\n",
       "  'block',\n",
       "  'we',\n",
       "  'se',\n",
       "  'them',\n",
       "  'as',\n",
       "  'addit',\n",
       "  'to',\n",
       "  'our',\n",
       "  'rich',\n",
       "  'divers',\n",
       "  'togeth',\n",
       "  'we',\n",
       "  'will',\n",
       "  'help',\n",
       "  'student',\n",
       "  'to',\n",
       "  'develop',\n",
       "  'to',\n",
       "  'their',\n",
       "  'full',\n",
       "  'potenti',\n",
       "  'creativ',\n",
       "  'probl',\n",
       "  'solv',\n",
       "  'compassionat',\n",
       "  'adult']]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes_list = list(cleaning_notes(train[\"project_essay_1\"][0:2]))\n",
    "notes_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def term_frequencies(notes_list):\n",
    "    \"\"\"\n",
    "    Compute the frequency of words\n",
    "    :param notes_list: A list of list\n",
    "    :return: A dict of words and frequencies\n",
    "    \"\"\"\n",
    "    \n",
    "    #length of the list\n",
    "    list_len = float(len(notes_list))\n",
    "\n",
    "    #words dict\n",
    "    words_dict = dict()\n",
    "\n",
    "    #cleaned elements\n",
    "    elements = cleaning_notes(notes_list)\n",
    "\n",
    "    #unlist the neasted list\n",
    "    elements = [x for sublist in elements for x in sublist]\n",
    "    \n",
    "    # add bigrams\n",
    "    bigrams = list([list(nltk.bigrams(note)) for note in notes_list])\n",
    "\n",
    "    l = [b[0]+\" \"+b[1] for b in bigrams]\n",
    "    print(l[:5])\n",
    "\n",
    "    #Count words\n",
    "    for word in elements:\n",
    "        if word not in words_dict:\n",
    "            words_dict[word] = 1.0 / list_len\n",
    "        else:\n",
    "            words_dict[word] += 1.0 /list_len\n",
    "    return words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate tuple (not \"str\") to tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-5cb8507e5bb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mterm_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotes_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-82-3803b973f498>\u001b[0m in \u001b[0;36mterm_frequencies\u001b[0;34m(notes_list)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mbigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnote\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnote\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnotes_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbigrams\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-82-3803b973f498>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mbigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnote\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnote\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnotes_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbigrams\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate tuple (not \"str\") to tuple"
     ]
    }
   ],
   "source": [
    "term_frequencies(notes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['most',\n",
       " 'of',\n",
       " 'my',\n",
       " 'kindergart',\n",
       " 'student',\n",
       " 'com',\n",
       " 'from',\n",
       " 'low',\n",
       " 'incom',\n",
       " 'household',\n",
       " 'and',\n",
       " 'are',\n",
       " 'consid',\n",
       " 'at',\n",
       " 'risk',\n",
       " 'thes',\n",
       " 'kid',\n",
       " 'walk',\n",
       " 'to',\n",
       " 'school',\n",
       " 'alongsid',\n",
       " 'their',\n",
       " 'parent',\n",
       " 'and',\n",
       " 'most',\n",
       " 'hav',\n",
       " 'nev',\n",
       " 'been',\n",
       " 'furth',\n",
       " 'than',\n",
       " 'walk',\n",
       " 'distanc',\n",
       " 'from',\n",
       " 'their',\n",
       " 'hous',\n",
       " 'for',\n",
       " 'of',\n",
       " 'my',\n",
       " 'student',\n",
       " 'english',\n",
       " 'is',\n",
       " 'not',\n",
       " 'their',\n",
       " 'first',\n",
       " 'languag',\n",
       " 'or',\n",
       " 'the',\n",
       " 'languag',\n",
       " 'spok',\n",
       " 'at',\n",
       " 'hom',\n",
       " 'nwhil',\n",
       " 'my',\n",
       " 'kindergart',\n",
       " 'kid',\n",
       " 'hav',\n",
       " 'mani',\n",
       " 'obstacl',\n",
       " 'in',\n",
       " 'front',\n",
       " 'of',\n",
       " 'them',\n",
       " 'they',\n",
       " 'com',\n",
       " 'to',\n",
       " 'school',\n",
       " 'each',\n",
       " 'day',\n",
       " 'excit',\n",
       " 'and',\n",
       " 'readi',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'most',\n",
       " 'student',\n",
       " 'start',\n",
       " 'the',\n",
       " 'year',\n",
       " 'out',\n",
       " 'nev',\n",
       " 'being',\n",
       " 'in',\n",
       " 'school',\n",
       " 'set',\n",
       " 'at',\n",
       " 'the',\n",
       " 'start',\n",
       " 'of',\n",
       " 'the',\n",
       " 'year',\n",
       " 'mani',\n",
       " 'had',\n",
       " 'nev',\n",
       " 'been',\n",
       " 'expos',\n",
       " 'to',\n",
       " 'letter',\n",
       " 'each',\n",
       " 'day',\n",
       " 'they',\n",
       " 'soak',\n",
       " 'up',\n",
       " 'mor',\n",
       " 'knowledg',\n",
       " 'and',\n",
       " 'tri',\n",
       " 'their',\n",
       " 'hard',\n",
       " 'to',\n",
       " 'succeed',\n",
       " 'they',\n",
       " 'are',\n",
       " 'high',\n",
       " 'motiv',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'new',\n",
       " 'thing',\n",
       " 'everi',\n",
       " 'day',\n",
       " 'we',\n",
       " 'are',\n",
       " 'halfway',\n",
       " 'through',\n",
       " 'the',\n",
       " 'year',\n",
       " 'and',\n",
       " 'they',\n",
       " 'are',\n",
       " 'start',\n",
       " 'to',\n",
       " 'tak',\n",
       " 'off',\n",
       " 'they',\n",
       " 'know',\n",
       " 'know',\n",
       " 'all',\n",
       " 'letter',\n",
       " 'som',\n",
       " 'sight',\n",
       " 'word',\n",
       " 'number',\n",
       " 'to',\n",
       " 'and',\n",
       " 'major',\n",
       " 'of',\n",
       " 'their',\n",
       " 'let',\n",
       " 'sound',\n",
       " 'becaus',\n",
       " 'of',\n",
       " 'their',\n",
       " 'hard',\n",
       " 'work',\n",
       " 'and',\n",
       " 'determin',\n",
       " 'am',\n",
       " 'excit',\n",
       " 'to',\n",
       " 'se',\n",
       " 'the',\n",
       " 'plac',\n",
       " 'we',\n",
       " 'will',\n",
       " 'go',\n",
       " 'from',\n",
       " 'her',\n",
       " 'our',\n",
       " 'elementari',\n",
       " 'school',\n",
       " 'is',\n",
       " 'cultur',\n",
       " 'rich',\n",
       " 'school',\n",
       " 'with',\n",
       " 'divers',\n",
       " 'popul',\n",
       " 'of',\n",
       " 'student',\n",
       " 'in',\n",
       " 'pre',\n",
       " 'through',\n",
       " 'sixth',\n",
       " 'grad',\n",
       " 'nour',\n",
       " 'titl',\n",
       " 'school',\n",
       " 'popul',\n",
       " 'has',\n",
       " 'of',\n",
       " 'student',\n",
       " 'qualifi',\n",
       " 'for',\n",
       " 'fre',\n",
       " 'or',\n",
       " 'reduc',\n",
       " 'price',\n",
       " 'lunch',\n",
       " 'and',\n",
       " 'high',\n",
       " 'concentr',\n",
       " 'of',\n",
       " 'english',\n",
       " 'learner',\n",
       " 'we',\n",
       " 'also',\n",
       " 'serv',\n",
       " 'two',\n",
       " 'fost',\n",
       " 'group',\n",
       " 'hom',\n",
       " 'for',\n",
       " 'temporari',\n",
       " 'and',\n",
       " 'long',\n",
       " 'term',\n",
       " 'plac',\n",
       " 'of',\n",
       " 'homeless',\n",
       " 'childr',\n",
       " 'nwe',\n",
       " 'do',\n",
       " 'not',\n",
       " 'se',\n",
       " 'thes',\n",
       " 'statistic',\n",
       " 'as',\n",
       " 'road',\n",
       " 'block',\n",
       " 'we',\n",
       " 'se',\n",
       " 'them',\n",
       " 'as',\n",
       " 'addit',\n",
       " 'to',\n",
       " 'our',\n",
       " 'rich',\n",
       " 'divers',\n",
       " 'togeth',\n",
       " 'we',\n",
       " 'will',\n",
       " 'help',\n",
       " 'student',\n",
       " 'to',\n",
       " 'develop',\n",
       " 'to',\n",
       " 'their',\n",
       " 'full',\n",
       " 'potenti',\n",
       " 'creativ',\n",
       " 'probl',\n",
       " 'solv',\n",
       " 'compassionat',\n",
       " 'adult']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = [elt for sublist in notes_list for elt in sublist]\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['most of', 'of my', 'my kindergart', 'kindergart student', 'student com']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = list(nltk.bigrams(notes_list[0]))\n",
    "bigrams[:5]\n",
    "l = [b[0]+\" \"+b[1] for b in bigrams]\n",
    "l[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf(d, d_all):\n",
    "    \"\"\"\n",
    "    Normalize the term frequencies of dict \"d\" by the frequencies of dict \"d_all\"\n",
    "    :param d: dict of term frquencies\n",
    "    :param d_all: dict of term frequencies\n",
    "    :return: A dict of normalized frequencies\n",
    "    \"\"\"\n",
    "    tfidf_d = {}\n",
    "    for key in d:\n",
    "        if key in d_all:\n",
    "            tfidf_d[key] = d[key] / d_all[key]\n",
    "        else:\n",
    "            tfidf_d[key] = d[key]\n",
    "    return tfidf_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_tokens(self, class0,\n",
    "                    classAll,\n",
    "                    dist_sep_bigrams=3,\n",
    "                    fold_enrichment=2,\n",
    "                    percent_filtering=None,\n",
    "                    occurence_bigram=1):\n",
    "    \"\"\"\n",
    "    Select the unique terms in the class and the most frequent terms by TFIDF\n",
    "    and the bigrams found by an anchored bigram finding strategy.\n",
    "    :param class0: list of notes from class0\n",
    "    :param classAll: list of notes from the rest of the classes\n",
    "    :param dist_sep_bigrams: optional distance between the anchor term and the second\n",
    "    part of the bigram\n",
    "    :param fold_enrichment: keep significant terms for the category of tickets\n",
    "    above fold_enrichment\n",
    "    :param percent_filtering: optional filtering threshold\n",
    "    :param occurence_bigram: optional filtering of bigrams\n",
    "    :return: list of tokens\n",
    "    \"\"\"\n",
    "    #Compute Term Frequencies\n",
    "    d       = self.term_frequencies(class0)\n",
    "    d_all   = self.term_frequencies(classAll)\n",
    "\n",
    "    #Compute TFIDF\n",
    "    tfidf = self.tfidf(d,d_all)\n",
    "\n",
    "    #Filter terms\n",
    "    filt_tfidf_d = {key: tfidf[key] for key in tfidf if tfidf[key] > fold_enrichment}\n",
    "\n",
    "    #Select unique terms for the category\n",
    "    d_set       = set(d.keys())\n",
    "    d_all_set   = set(d_all.keys())\n",
    "\n",
    "    uniq_terms  = [x for x in d_set if x not in d_all_set]\n",
    "    uniq_term_d = {key: d[key] for key in uniq_terms}\n",
    "\n",
    "    #update dict of tokens\n",
    "    filt_tfidf_d.update(uniq_term_d)\n",
    "\n",
    "    #The set of tokens for the class\n",
    "    tokens = set(filt_tfidf_d.keys())\n",
    "\n",
    "    #For each notes, select the terms in the token set\n",
    "    enriched_elements = []\n",
    "\n",
    "    for note in class0:\n",
    "        words = self.split_and_clean(note)\n",
    "        #Filter\n",
    "        words = [x for x in words if x in tokens]\n",
    "        #Collect words and enrichments in a tuple\n",
    "        enriched_elements.append([(key, round(filt_tfidf_d[key], 1)) for key in words])\n",
    "\n",
    "    #optionally filtering enriched_element above a certain threshold\n",
    "    filtered_enriched_elements = []\n",
    "    if percent_filtering:\n",
    "        for e in enriched_elements:\n",
    "            ratios = [x[1] for x in e]\n",
    "            filtered_enriched_elements.append(\n",
    "                [x for x in e if x[1] > np.percentile(ratios, percent_filtering)]\n",
    "            )\n",
    "    else:\n",
    "        filtered_enriched_elements = enriched_elements\n",
    "\n",
    "    #Select bigrams from the filtered_enriched_elements\n",
    "    list_of_dict = []\n",
    "\n",
    "    for element, sentence in zip(\n",
    "        filtered_enriched_elements,\n",
    "        self.cleaning_notes(class0)\n",
    "    ):\n",
    "        list_of_dict.append(\n",
    "            self.find_bigrams(element, sentence, d, dist_sep=dist_sep_bigrams)\n",
    "        )\n",
    "\n",
    "    bigrams = [d.keys() for d in list_of_dict]\n",
    "    bigrams = [l for sublist in bigrams for l in sublist]\n",
    "\n",
    "    #print(bigrams)\n",
    "\n",
    "    bigram_counts = [(x, bigrams.count(x)) for x in set(bigrams)]\n",
    "\n",
    "    if occurence_bigram:\n",
    "        filt_bigram_count = []\n",
    "        for b in bigram_counts:\n",
    "            if b[1] > occurence_bigram:\n",
    "                filt_bigram_count.append(b[0])\n",
    "        bigram_counts = filt_bigram_count\n",
    "    #print(bigram_counts)\n",
    "    #print(tokens)\n",
    "    merge = sorted(list(tokens)) + sorted(bigram_counts)\n",
    "    return merge\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'abstractClasses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-0b15ee50dd54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTokenizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabstractClasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTokenizing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masaph_path_firstname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'abstractClasses' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import FrenchStemmer, EnglishStemmer, GermanStemmer\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from six import string_types\n",
    "import os\n",
    "\n",
    "\n",
    "class Tokenizing(abstractClasses.Tokenizing):\n",
    "    def __init__(self, data):\n",
    "        self.names_file = os.path.abspath(names_file)\n",
    "        self.data = data\n",
    "        firstname_df = pd.read_csv(self.names_file, sep='\\t', encoding='latin-1')\n",
    "        firstnames = list(set(firstname_df.preusuel))\n",
    "        # cleaning accents\n",
    "\n",
    "        self.firstnames_set = set(unicodedata.normalize('NFD', nom)\n",
    "                                  .encode('ASCII', 'ignore')\n",
    "                                  .lower()\n",
    "                                  for nom in firstnames if isinstance(nom, string_types))\n",
    "\n",
    "        # Regex pattern to match punctuation signs\n",
    "        self.regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "        # Initate stemmers\n",
    "        self.french_stem = FrenchStemmer(ignore_stopwords=True)\n",
    "        self.english_stem = EnglishStemmer(ignore_stopwords=True)\n",
    "        self.german_stem = GermanStemmer(ignore_stopwords=True)\n",
    "\n",
    "        # Initiate tokens list\n",
    "        self.tokens = []\n",
    "\n",
    "\n",
    "\n",
    "    def stem_word(self,word):\n",
    "        \"\"\"\n",
    "        Remove the suffix of word using a stemmer from the most probable\n",
    "        to the least probable language.\n",
    "        :param word: string\n",
    "        :return: string\n",
    "        \"\"\"\n",
    "        y = self.french_stem.stem(word)\n",
    "\n",
    "        #if the word is not french, it may be english\n",
    "        if word != y:\n",
    "            return y\n",
    "        else:\n",
    "            w = self.english_stem.stem(word)\n",
    "            if word != w:\n",
    "                return w\n",
    "            else:\n",
    "                #if not english it may be german\n",
    "                z = self.german_stem.stem(word)\n",
    "                if word != z:\n",
    "                    return z\n",
    "                else:\n",
    "                    return word\n",
    "\n",
    "    \n",
    "\n",
    "    def cleaning_notes(self, note_list):\n",
    "        \"\"\"\n",
    "        Processing notes text before counting\n",
    "        :param note_list: The list of notes from self.data\n",
    "        :return: A list of list of words\n",
    "        \"\"\"\n",
    "        #Set strings as unicode\n",
    "        u_note_list = [str(x) for x in note_list]\n",
    "\n",
    "        return map(self.split_and_clean, u_note_list)\n",
    "\n",
    "    def term_frequencies(self, notes_list):\n",
    "        \"\"\"\n",
    "        Compute the frequency of words\n",
    "        :param notes_list: A list of list\n",
    "        :return: A dict of words and frequencies\n",
    "        \"\"\"\n",
    "        #length of the list\n",
    "        list_len = float(len(notes_list))\n",
    "\n",
    "        #words dict\n",
    "        words_dict = dict()\n",
    "\n",
    "        #cleaned elements\n",
    "        elements = self.cleaning_notes(notes_list)\n",
    "\n",
    "        #unlist the neasted list\n",
    "        elements = [x for sublist in elements for x in sublist]\n",
    "\n",
    "        #Count words\n",
    "        for word in elements:\n",
    "            if word not in words_dict:\n",
    "                words_dict[word] = 1.0 / list_len\n",
    "            else:\n",
    "                words_dict[word] += 1.0 /list_len\n",
    "        return words_dict\n",
    "\n",
    "    def tfidf(self, d, d_all):\n",
    "        \"\"\"\n",
    "        Normalize the term frequencies of dict \"d\" by the frequencies of dict \"d_all\"\n",
    "        :param d: dict of term frquencies\n",
    "        :param d_all: dict of term frequencies\n",
    "        :return: A dict of normalized frequencies\n",
    "        \"\"\"\n",
    "        tfidf_d = {}\n",
    "        for key in d:\n",
    "            if key in d_all:\n",
    "                tfidf_d[key] = d[key] / d_all[key]\n",
    "            else:\n",
    "                tfidf_d[key] = d[key]\n",
    "        return tfidf_d\n",
    "\n",
    "    def find_bigrams(self, elements, sentence, d, dist_sep):\n",
    "        \"\"\"\n",
    "        elements are used as anchoring term to find a frequent term nearby in the sentence\n",
    "        :param elements: list of tuples (word, enrichment)\n",
    "        :param sentence: list of words\n",
    "        :param d: dict of frequencies\n",
    "        :param dist_sep: separation distance to find the second part of the bigram\n",
    "        :return: dict of bigrams and frequencies\n",
    "        \"\"\"\n",
    "        position_anchors = [sentence.index(e[0]) for e in elements]\n",
    "        bigrams = dict()\n",
    "        second_element_list = []\n",
    "\n",
    "        for x in position_anchors:\n",
    "            for y in range(x-dist_sep, x+dist_sep):\n",
    "                #select the most frequent term in d near the anchor\n",
    "                max_freq = 0\n",
    "                if len(sentence) > y > 0 :\n",
    "                    if y != x  and d[sentence[y-1]] > max_freq:\n",
    "                        max_freq = d[sentence[y-1]]\n",
    "                        second_element_list.append(sentence[y-1])\n",
    "\n",
    "\n",
    "\n",
    "        for anchor, complmt in zip(position_anchors,second_element_list):\n",
    "            #store in alphabetical order\n",
    "            if sentence[anchor] < complmt:\n",
    "                key = (sentence[anchor], complmt)\n",
    "            else:\n",
    "                key = (complmt, sentence[anchor])\n",
    "\n",
    "            if key not in bigrams:\n",
    "                bigrams[key] = 1.0\n",
    "            else:\n",
    "                bigrams[key] += 1.0\n",
    "\n",
    "        return bigrams\n",
    "\n",
    "    def find_tokens(self, class0,\n",
    "                    classAll,\n",
    "                    dist_sep_bigrams=3,\n",
    "                    fold_enrichment=2,\n",
    "                    percent_filtering=None,\n",
    "                    occurence_bigram=1):\n",
    "        \"\"\"\n",
    "        Select the unique terms in the class and the most frequent terms by TFIDF\n",
    "        and the bigrams found by an anchored bigram finding strategy.\n",
    "        :param class0: list of notes from class0\n",
    "        :param classAll: list of notes from the rest of the classes\n",
    "        :param dist_sep_bigrams: optional distance between the anchor term and the second\n",
    "        part of the bigram\n",
    "        :param fold_enrichment: keep significant terms for the category of tickets\n",
    "        above fold_enrichment\n",
    "        :param percent_filtering: optional filtering threshold\n",
    "        :param occurence_bigram: optional filtering of bigrams\n",
    "        :return: list of tokens\n",
    "        \"\"\"\n",
    "        #Compute Term Frequencies\n",
    "        d       = self.term_frequencies(class0)\n",
    "        d_all   = self.term_frequencies(classAll)\n",
    "\n",
    "        #Compute TFIDF\n",
    "        tfidf = self.tfidf(d,d_all)\n",
    "\n",
    "        #Filter terms\n",
    "        filt_tfidf_d = {key: tfidf[key] for key in tfidf if tfidf[key] > fold_enrichment}\n",
    "\n",
    "        #Select unique terms for the category\n",
    "        d_set       = set(d.keys())\n",
    "        d_all_set   = set(d_all.keys())\n",
    "\n",
    "        uniq_terms  = [x for x in d_set if x not in d_all_set]\n",
    "        uniq_term_d = {key: d[key] for key in uniq_terms}\n",
    "\n",
    "        #update dict of tokens\n",
    "        filt_tfidf_d.update(uniq_term_d)\n",
    "\n",
    "        #The set of tokens for the class\n",
    "        tokens = set(filt_tfidf_d.keys())\n",
    "\n",
    "        #For each notes, select the terms in the token set\n",
    "        enriched_elements = []\n",
    "\n",
    "        for note in class0:\n",
    "            words = self.split_and_clean(note)\n",
    "            #Filter\n",
    "            words = [x for x in words if x in tokens]\n",
    "            #Collect words and enrichments in a tuple\n",
    "            enriched_elements.append([(key, round(filt_tfidf_d[key], 1)) for key in words])\n",
    "\n",
    "        #optionally filtering enriched_element above a certain threshold\n",
    "        filtered_enriched_elements = []\n",
    "        if percent_filtering:\n",
    "            for e in enriched_elements:\n",
    "                ratios = [x[1] for x in e]\n",
    "                filtered_enriched_elements.append(\n",
    "                    [x for x in e if x[1] > np.percentile(ratios, percent_filtering)]\n",
    "                )\n",
    "        else:\n",
    "            filtered_enriched_elements = enriched_elements\n",
    "\n",
    "        #Select bigrams from the filtered_enriched_elements\n",
    "        list_of_dict = []\n",
    "\n",
    "        for element, sentence in zip(\n",
    "            filtered_enriched_elements,\n",
    "            self.cleaning_notes(class0)\n",
    "        ):\n",
    "            list_of_dict.append(\n",
    "                self.find_bigrams(element, sentence, d, dist_sep=dist_sep_bigrams)\n",
    "            )\n",
    "\n",
    "        bigrams = [d.keys() for d in list_of_dict]\n",
    "        bigrams = [l for sublist in bigrams for l in sublist]\n",
    "\n",
    "        #print(bigrams)\n",
    "\n",
    "        bigram_counts = [(x, bigrams.count(x)) for x in set(bigrams)]\n",
    "\n",
    "        if occurence_bigram:\n",
    "            filt_bigram_count = []\n",
    "            for b in bigram_counts:\n",
    "                if b[1] > occurence_bigram:\n",
    "                    filt_bigram_count.append(b[0])\n",
    "            bigram_counts = filt_bigram_count\n",
    "        #print(bigram_counts)\n",
    "        #print(tokens)\n",
    "        merge = sorted(list(tokens)) + sorted(bigram_counts)\n",
    "        return merge\n",
    "\n",
    "    def fill_count_matrix(self,df, mat, tokens):\n",
    "        \"\"\"\n",
    "        iterate through df to fill the matrix mat\n",
    "        :param df: a dataframe\n",
    "        :param mat: numpy array\n",
    "        :param tokens: list of sorted tokens\n",
    "        :return: numpy array\n",
    "        \"\"\"\n",
    "        #ToDo: Add notes_list as a class variable\n",
    "        notes_list = self.cleaning_notes(df.notes.tolist())\n",
    "\n",
    "        for i, note in enumerate(notes_list):\n",
    "            for j, token in enumerate(tokens):\n",
    "                if type(token) is tuple:\n",
    "                    if token[0] in note and token[1] in note:\n",
    "                        #ToDo: compute the count of bigrams\n",
    "                        mat[i,j] = 1.0\n",
    "                else:\n",
    "                    mat[i,j] = float(note.count(token))\n",
    "        return mat\n",
    "\n",
    "    def select_tokens(self, **kwargs):\n",
    "        \"\"\"\n",
    "        iterate through each class to collect tokens and build a matrix\n",
    "\n",
    "        :param kwargs: dist_sep_bigrams=3,\n",
    "                    fold_enrichment=2,\n",
    "                    percent_filtering=None,\n",
    "                    occurence_bigram=1\n",
    "        :return: numpy array\n",
    "        \"\"\"\n",
    "        nb_classes = np.max(self.data.category_number)\n",
    "\n",
    "        token_set = set()\n",
    "\n",
    "        for i in range(1, nb_classes + 1):\n",
    "            class0      = self.data.loc[self.data.category_number == i, 'notes'].tolist()\n",
    "            classAll    = self.data.loc[self.data.category_number != i, 'notes'].tolist()\n",
    "\n",
    "            selected_tokens = self.find_tokens(class0,classAll, **kwargs)\n",
    "            token_set.update(set(selected_tokens))\n",
    "\n",
    "        self.tokens = list(token_set)\n",
    "        #print(self.tokens)\n",
    "        self.tokens.sort(key= lambda x: str(x[0]))  # messy, python 3 doesn't like to sort tuples\n",
    "\n",
    "        mat = np.zeros((len(self.data), len(self.tokens)))\n",
    "\n",
    "        X = self.fill_count_matrix(self.data, mat, self.tokens)\n",
    "        return X\n",
    "\n",
    "    def fit(self, **kwargs):\n",
    "        \"\"\"\n",
    "        The main method to convert tickets notes into a numpy array\n",
    "\n",
    "        :param dist_sep_bigrams: max number of words between elements of the bigram\n",
    "        :param fold_enrichment: minimum TFIDF enrichment to keep the tokens\n",
    "        :param percent_filtering: quantile value in percent to consider a word as\n",
    "        the partner of the anchor term in a bigram\n",
    "        :param occurence_bigram: Number of time the bigram should occurs in\n",
    "        the category to keep it as a feature\n",
    "        :return: numpy array samples x tokens, numpy array of category_numbers\n",
    "        \"\"\"\n",
    "        #Annotate the DataFrame with its category numbers\n",
    "        #self.annotate_classes(sel_cat_list=sel_cat_list, cat_col=cat_col)\n",
    "\n",
    "        #Select tokens\n",
    "        self.X = self.select_tokens(**kwargs)\n",
    "\n",
    "        self.y = self.data.category_number.as_matrix()\n",
    "\n",
    "        return self.X, self.y\n",
    "\n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        Generate a matrix from the notes field of the DataFrame with the tokens found during fit\n",
    "        :param df: DataFrame\n",
    "        :return: numpy array\n",
    "        \"\"\"\n",
    "        mat = np.zeros((len(df), len(self.tokens)))\n",
    "        X = self.fill_count_matrix(df, mat, self.tokens)\n",
    "        return X\n",
    "\n",
    "    def tokenize(self, **kwargs):\n",
    "        return self.fit(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
